{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea2c52-3ead-4484-8a13-59ff459b0f78",
   "metadata": {
    "id": "e3ea2c52-3ead-4484-8a13-59ff459b0f78"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import requests\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble, linear_model, metrics, model_selection\n",
    "from sklearn import pipeline, preprocessing, svm, tree, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b241c6-de2e-4c9a-ac4f-6c5a010e2929",
   "metadata": {
    "id": "a9b241c6-de2e-4c9a-ac4f-6c5a010e2929",
    "tags": []
   },
   "source": [
    "# MNIST Dataset\n",
    "\n",
    "The original [MNIST](http://yann.lecun.com/exdb/mnist/) dataset consists of 70000 28x28 black and white images in 10 classes. There are 60000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a812f-ac93-4d77-871d-0d282f51423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might be different if using Colab or Kaggle\n",
    "PROJECT_ROOT_DIR = pathlib.Path(\"..\")\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT_DIR / \"data\" / \"mnist\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT_DIR / \"results\" / \"mnist\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d2238-7283-441a-b6d8-4ba3c4c682f0",
   "metadata": {
    "id": "e68d2238-7283-441a-b6d8-4ba3c4c682f0"
   },
   "source": [
    "### Download and extract the data (if using Colab or Kaggle!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09916110-f2d8-448c-86ee-4be9ae5be169",
   "metadata": {
    "id": "09916110-f2d8-448c-86ee-4be9ae5be169",
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"https://github.com/KAUST-Academy/practical-tools-for-machine-learning/blob/october-2022/data/mnist/mnist.parquet?raw=true\"\n",
    "\n",
    "with open(DATA_DIR / \"mnist.parquet\", 'wb') as f:\n",
    "    response = requests.get(URL)\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc24c9-260e-4840-8872-723f0adc6f41",
   "metadata": {
    "id": "02cc24c9-260e-4840-8872-723f0adc6f41"
   },
   "source": [
    "### Load the data\n",
    "\n",
    "We will load the data using the [Pandas](https://pandas.pydata.org/) library. Highly recommend the most recent edition of [*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781491957653/) by Pandas creator Wes Mckinney for anyone interested in learning how to use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fded7d-02a7-48c3-99e4-d19d001c8ccf",
   "metadata": {
    "id": "c1fded7d-02a7-48c3-99e4-d19d001c8ccf"
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(DATA_DIR / \"mnist.parquet\")\n",
    "features = data.drop(\"label\", axis=1)\n",
    "target = data.loc[:, \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53a9c4-550d-4577-bf0f-bace74843784",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b53a9c4-550d-4577-bf0f-bace74843784",
    "outputId": "f8441f2a-1945-40fb-9409-bf290c79afd2"
   },
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e51a47-4730-410d-b5d6-4278a48ff5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2212c9-f227-4b8f-ac41-6deb6f714976",
   "metadata": {
    "id": "7a2212c9-f227-4b8f-ac41-6deb6f714976"
   },
   "source": [
    "# Creating a Test Dataset\n",
    "\n",
    "Before we look at the data any further, we need to create a test set, put it aside, and never look at it (until we are ready to test our trainined machine learning model!). Why? We don't want our machine learning model to memorize our dataset (this is called overfitting). Instead we want a model that will generalize well (i.e., make good predictions) for inputs that it didn't see during training. To do this we hold split our dataset into training and testing datasets. The training dataset will be used to train our machine learning model(s) and the testing dataset will be used to make a final evaluation of our machine learning model(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791e74d-cddd-4383-8a6c-5b80df4a5daf",
   "metadata": {
    "id": "f791e74d-cddd-4383-8a6c-5b80df4a5daf"
   },
   "outputs": [],
   "source": [
    "model_selection.train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35bf11-0fa5-4879-bbda-e976244ec929",
   "metadata": {
    "id": "6b35bf11-0fa5-4879-bbda-e976244ec929"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SEED_GENERATOR = np.random.RandomState(SEED)\n",
    "\n",
    "\n",
    "def generate_seed():\n",
    "    return SEED_GENERATOR.randint(np.iinfo(\"uint16\").max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff5a36-07c7-404f-916b-260c789aff91",
   "metadata": {
    "id": "beff5a36-07c7-404f-916b-260c789aff91"
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and testing data\n",
    "_seed = generate_seed()\n",
    "_random_state = np.random.RandomState(_seed)\n",
    "\n",
    "train_features, test_features, train_target, test_target = model_selection.train_test_split(\n",
    "    features,\n",
    "    target,\n",
    "    test_size=1e-1,\n",
    "    random_state=_random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed741c-996c-4df3-8680-49f14cc0dcf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9ed741c-996c-4df3-8680-49f14cc0dcf0",
    "outputId": "9d66db77-3063-4cfb-8b92-05118d8ddcf8"
   },
   "outputs": [],
   "source": [
    "train_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f35a7-604c-486b-8f36-284b909c64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090af9ae-e36c-4c5d-ac2d-25e1ea1c34df",
   "metadata": {
    "id": "090af9ae-e36c-4c5d-ac2d-25e1ea1c34df"
   },
   "source": [
    "Again, if you want to you can write out the train and test sets to disk to avoid having to recreate them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede1948-a85a-450f-8c0b-8c18b56ed4d3",
   "metadata": {
    "id": "6ede1948-a85a-450f-8c0b-8c18b56ed4d3"
   },
   "outputs": [],
   "source": [
    "_ = (train_features.join(train_target)\n",
    "                   .to_parquet(DATA_DIR / \"train.parquet\", index=False))\n",
    "\n",
    "_ = (test_features.join(test_target)\n",
    "                   .to_parquet(DATA_DIR / \"test.parquet\", index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b292be-f7b9-4958-a6d0-c957ebcc50a5",
   "metadata": {
    "id": "01b292be-f7b9-4958-a6d0-c957ebcc50a5"
   },
   "source": [
    "# Select and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627d70a-418f-49ed-8449-5ba412a77d2c",
   "metadata": {
    "id": "e627d70a-418f-49ed-8449-5ba412a77d2c"
   },
   "source": [
    "## Training and evaluating on the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab8454-92c2-46f9-9c60-ac0f5855bedd",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335899f-8bbd-4bb8-a411-d68da1ad8803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svm.LinearSVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792c574-ebd3-4d5c-9ed3-5b8db288b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "\n",
    "_classifier_hyperparameters = {\n",
    "    \"dual\": False,\n",
    "    \"tol\": 0.0001,\n",
    "    \"C\": 1.0,\n",
    "    \"fit_intercept\": True,\n",
    "    \"class_weight\": None,\n",
    "    \"verbose\": 0,\n",
    "    \"random_state\": np.random.RandomState(_seed),\n",
    "    \"max_iter\": 1000,\n",
    "}\n",
    "\n",
    "ml_pipeline = pipeline.make_pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    svm.LinearSVC(**_classifier_hyperparameters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc1e21-8f3f-410c-a0cf-4aa981e59891",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ml_pipeline.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4d7c8-cac3-4d72-ac9c-41a0df173775",
   "metadata": {},
   "source": [
    "After training the pipeline we can assess its performance on the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7c6d8-a208-4e04-bda7-62c1f35bdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "_train_predictions = ml_pipeline.predict(train_features)\n",
    "\n",
    "# report the accuracy on the training data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1b1b0-dfb2-451b-99dd-c993d22c5e6e",
   "metadata": {},
   "source": [
    "...and then assess its performance on new data using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c53d46-5648-4e24-be70-356971df9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_FOLDS = 3\n",
    "\n",
    "_train_predictions = model_selection.cross_val_predict(\n",
    "    ml_pipeline,\n",
    "    X=train_features,\n",
    "    y=train_target,\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# report the accuracy on the cv data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8999df08-6848-44a8-9699-47c21ca0b5c8",
   "metadata": {},
   "source": [
    "You can also use SGD to fit linear support vector machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60b224-6d20-4a71-92fe-f5cdc7be9401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_model.SGDClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f86923-ce8c-4f57-b8d7-7e7d1d710a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "\n",
    "_classifier_hyperparameters = {\n",
    "    \"loss\": \"hinge\",\n",
    "    \"fit_intercept\": True,\n",
    "    \"verbose\": 0,\n",
    "    \"random_state\": np.random.RandomState(_seed),\n",
    "}\n",
    "\n",
    "ml_pipeline = pipeline.make_pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    linear_model.SGDClassifier(**_classifier_hyperparameters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2f25a-e712-466e-a58d-3ac7204f5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ml_pipeline.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a794672-7edc-4d89-9974-7ddabe744cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "_train_predictions = ml_pipeline.predict(train_features)\n",
    "\n",
    "# report the accuracy on the training data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726094fa-65cf-4188-8283-ce62d4da5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_predictions = model_selection.cross_val_predict(\n",
    "    ml_pipeline,\n",
    "    X=train_features,\n",
    "    y=train_target,\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# report the accuracy on the cv data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b32ff9-ae55-46c4-ad21-528413893ac0",
   "metadata": {},
   "source": [
    "### Exercise: Early Stopping\n",
    "\n",
    "Read through the documentation for the [`linear_model.SGDClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) and implement a linear SVM and train the model using early stopping to control overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c8b6b-fac7-4a38-b7fe-5e2a8c99cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.SGDClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5815d4-941e-4790-8426-ec9e56b531aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be202dc8-9556-4c87-a70a-06b1df636521",
   "metadata": {
    "id": "be202dc8-9556-4c87-a70a-06b1df636521"
   },
   "source": [
    "### Decision Trees\n",
    "\n",
    "[Decision Trees](https://scikit-learn.org/stable/modules/tree.html) are a non-parametric supervised learning method used for [classification](https://scikit-learn.org/stable/modules/tree.html#tree-classification) and [regression](https://scikit-learn.org/stable/modules/tree.html#tree-regression). The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cd1e0-6abe-4565-a1a3-1ebc279eb176",
   "metadata": {
    "id": "163cd1e0-6abe-4565-a1a3-1ebc279eb176"
   },
   "outputs": [],
   "source": [
    "tree.DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62bbb1-9a89-4b3f-93db-f55cb6db0ef8",
   "metadata": {
    "id": "2b62bbb1-9a89-4b3f-93db-f55cb6db0ef8"
   },
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "\n",
    "_classifier_hyperparameters = {\n",
    "    \"random_state\": np.random.RandomState(_seed)\n",
    "}\n",
    "\n",
    "estimator = tree.DecisionTreeClassifier(**_classifier_hyperparameters)\n",
    "\n",
    "# here we fit using the raw training features\n",
    "_ = estimator.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c2bc4-6ead-401d-9b72-ee84935ccbab",
   "metadata": {
    "id": "b64c2bc4-6ead-401d-9b72-ee84935ccbab"
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "_train_predictions = estimator.predict(train_features)\n",
    "\n",
    "# report the accuracy on the training data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21482c8-095e-4cc5-92a4-c01f1c61893e",
   "metadata": {
    "id": "e21482c8-095e-4cc5-92a4-c01f1c61893e"
   },
   "source": [
    "Wait, what!? No error at all? Could this model really be absolutely perfect? Unfortunately it is much more likely that the model has badly overfit the training data. How can you be sure? As we saw earlier, you don’t want to touch the testing dataset until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part of it for model validation.\n",
    "\n",
    "The following code use Scikit-Learn [`model_selection.cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) to randomly split the training set into `CV_FOLDS` distinct subsets called folds, then it trains and evaluates our model `CV_FOLDS` times, picking a different fold for evaluation every time and training on the other `CV_FOLDS-1` folds. The result is an array containing the `CV_FOLDS` evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cc464-e680-4933-b477-9aeed953ae3a",
   "metadata": {
    "id": "e51cc464-e680-4933-b477-9aeed953ae3a"
   },
   "outputs": [],
   "source": [
    "estimator_scores = model_selection.cross_val_score(\n",
    "    estimator,\n",
    "    X=train_features,\n",
    "    y=train_target,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f0397-ca37-4167-89d7-2af077374cc0",
   "metadata": {
    "id": "b26f0397-ca37-4167-89d7-2af077374cc0"
   },
   "outputs": [],
   "source": [
    "estimator_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa85cb1-daf3-4f3d-80e1-b22831759054",
   "metadata": {},
   "source": [
    "The following code use Scikit-Learn [`model_selection.cross_val_predict`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) to randomly split the training set into `CV_FOLDS` distinct subsets called folds, then it trains the model and makes predictions using the trained model `CV_FOLDS` times, picking a different fold to use when making prediction every time and training on the other `CV_FOLDS-1` folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4fc15-155d-4643-8a37-fddfdfa3fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_predictions = model_selection.cross_val_predict(\n",
    "    estimator,\n",
    "    X=train_features,\n",
    "    y=train_target,\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# report the accuracy on the training data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481229f5-1d1a-4e4b-94eb-a7e5c7d8a28b",
   "metadata": {
    "id": "481229f5-1d1a-4e4b-94eb-a7e5c7d8a28b"
   },
   "source": [
    "### Understanding Feature Importance\n",
    "\n",
    "One of the nice features of decision trees is that they provide a way to measure the importance of each of feature. Understanding feature importance is a topic all unto itself. If you are interested in pulling this thread, then I recommend that you start with [SHapley Additive Explanations (SHAP)](https://shap.readthedocs.io/en/latest/index.html) and then take a look through [*Interpretable Machine Learning*](https://christophm.github.io/interpretable-ml-book/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7474a-523f-4af3-b52c-210860f6c83f",
   "metadata": {
    "id": "51d7474a-523f-4af3-b52c-210860f6c83f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f85cf-f200-4d6c-b09b-07656b9e8df6",
   "metadata": {
    "id": "815f85cf-f200-4d6c-b09b-07656b9e8df6"
   },
   "outputs": [],
   "source": [
    "is_positive = estimator.feature_importances_ > 0\n",
    "is_positive.sum() / estimator.feature_importances_.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e7445-5e4c-4dda-b80e-4b124f840bfb",
   "metadata": {
    "id": "176e7445-5e4c-4dda-b80e-4b124f840bfb"
   },
   "source": [
    "Because our features are pixels we can reshape and plot the features to gain some insight into what might be driving feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5272f2-4e56-4f39-9dbe-82a2cdef218b",
   "metadata": {
    "id": "ef5272f2-4e56-4f39-9dbe-82a2cdef218b"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "_average_pixel_importances = (\n",
    "    estimator.feature_importances_\n",
    "             .reshape((1, 28, 28))\n",
    "             .mean(axis=0)\n",
    ")\n",
    "plt.imshow(_average_pixel_importances)\n",
    "plt.title(\"Average Pixel Importance\", fontsize=20)\n",
    "\n",
    "# create a colorbar\n",
    "colorbar = plt.colorbar(ticks=[_average_pixel_importances.min(), _average_pixel_importances.max()])\n",
    "_ = (colorbar.ax\n",
    "             .set_yticklabels([\"Not important\", \"Very important\"], fontsize=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20d47c-5286-4c07-8139-574c3d6be82c",
   "metadata": {
    "id": "0a20d47c-5286-4c07-8139-574c3d6be82c"
   },
   "source": [
    "### Exercise: Feature Extraction\n",
    "\n",
    "The fair number of features seem to be unimportant which suggests that perhaps we should look at ways to extract the most important features prior to fitting our decision tree model. How could we extract the most relevant features prior to fitting our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a03808-6cab-4f4d-89fb-30d001f4f6ab",
   "metadata": {
    "id": "d6a03808-6cab-4f4d-89fb-30d001f4f6ab"
   },
   "source": [
    "### Exercise: Regularizing Decision Trees\n",
    "\n",
    "Our decision tree classifier appears to be overfitting. We need to regularize it. Decreasing `max_*` parameters and increasing `min_*` parameters will increase the amount of regularization applied to the model and will help reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4c7b2-1fc9-4787-89ec-05e32fa964de",
   "metadata": {
    "id": "9ab4c7b2-1fc9-4787-89ec-05e32fa964de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree.DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2629805-614f-44ff-af20-7a573f35a263",
   "metadata": {
    "id": "b2629805-614f-44ff-af20-7a573f35a263"
   },
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "\n",
    "_classifier_hyperparameters = {\n",
    "    \"criterion\": \"entropy\",\n",
    "    \"max_depth\": 32,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"min_samples_leaf\": 1e-3,\n",
    "    \"min_samples_split\": 2e-3,\n",
    "    \"random_state\": np.random.RandomState(_seed)\n",
    "}\n",
    "\n",
    "estimator = tree.DecisionTreeClassifier(**_classifier_hyperparameters)\n",
    "_ = estimator.fit(train_features, train_target)\n",
    "\n",
    "# make predictions using cv\n",
    "_train_predictions = model_selection.cross_val_predict(\n",
    "    estimator,\n",
    "    X=train_features,\n",
    "    y=train_target,\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# create a classification report\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d2251-e5af-43a4-b6e4-4649d41f228a",
   "metadata": {
    "id": "5d3d2251-e5af-43a4-b6e4-4649d41f228a"
   },
   "source": [
    "### Exercise: Error Analysis\n",
    "\n",
    "Compute the normalized confusion matrix for your decision tree classifier and plot it. Do you notice any patterns? What are the three classes for which your decision tree classifier peforms the worst?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2376ee8-1afe-4b61-a2eb-c43c5bd2bf93",
   "metadata": {
    "id": "d2376ee8-1afe-4b61-a2eb-c43c5bd2bf93"
   },
   "outputs": [],
   "source": [
    "# insert code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9df98-2d98-42ef-aef6-1aa73321c757",
   "metadata": {
    "id": "45a9df98-2d98-42ef-aef6-1aa73321c757"
   },
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "Building a model on top of many other models is called [ensemble](https://scikit-learn.org/stable/modules/ensemble.html) learning and it is often a great approach to improve the predictions of your machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1953f-5ae2-4bb2-b96a-b483a8fc972b",
   "metadata": {
    "id": "cdc1953f-5ae2-4bb2-b96a-b483a8fc972b"
   },
   "source": [
    "#### Random Forests\n",
    "\n",
    "Let’s try the [`ensemble.RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Random forests work by training many decision trees on random subsets of the features, then averaging the predictions made by each of the decision trees to arrive at an overall prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501b50b-5f60-4f2c-9698-5d517223474e",
   "metadata": {
    "id": "1501b50b-5f60-4f2c-9698-5d517223474e"
   },
   "outputs": [],
   "source": [
    "ensemble.RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90a8fe-9956-4e9d-ab44-11d915ff9b84",
   "metadata": {
    "id": "ff90a8fe-9956-4e9d-ab44-11d915ff9b84"
   },
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "_classifier_hyperparameters = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": np.random.RandomState(_seed)\n",
    "}\n",
    "\n",
    "estimator = ensemble.RandomForestClassifier(**_classifier_hyperparameters)\n",
    "_ = estimator.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b4fc9-d9fb-47de-a85e-0f1af0e62637",
   "metadata": {
    "id": "053b4fc9-d9fb-47de-a85e-0f1af0e62637"
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "_train_predictions = estimator.predict(train_features)\n",
    "\n",
    "# generate a classification report\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad4c91-d869-434f-b316-3aa66bcc06b0",
   "metadata": {
    "id": "d5ad4c91-d869-434f-b316-3aa66bcc06b0"
   },
   "source": [
    "##### Visualizing Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9091f64-c2fa-43da-94f3-3c482342d382",
   "metadata": {
    "id": "f9091f64-c2fa-43da-94f3-3c482342d382"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "_average_pixel_importances = (\n",
    "    estimator.feature_importances_\n",
    "             .reshape((1, 28, 28))\n",
    "             .mean(axis=0)\n",
    ")\n",
    "plt.imshow(_average_pixel_importances)\n",
    "plt.title(\"Average Pixel Importance\", fontsize=20)\n",
    "\n",
    "# create a colorbar\n",
    "colorbar = plt.colorbar(ticks=[_average_pixel_importances.min(), _average_pixel_importances.max()])\n",
    "_ = (colorbar.ax\n",
    "             .set_yticklabels([\"Not important\", \"Very important\"], fontsize=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b5cc1-598d-468e-9762-848f3c116a9b",
   "metadata": {
    "id": "4b5b5cc1-598d-468e-9762-848f3c116a9b"
   },
   "source": [
    "Again you can use $k$-fold CV to estimate the validation error for your random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595a6ef-86ac-4f1b-9a43-cc69449a82cb",
   "metadata": {
    "id": "f595a6ef-86ac-4f1b-9a43-cc69449a82cb"
   },
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "_classifier_hyperparameters = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": np.random.RandomState(_seed)\n",
    "}\n",
    "estimator = ensemble.RandomForestClassifier(**_classifier_hyperparameters)\n",
    "\n",
    "# make predictions using cv\n",
    "_train_predictions = model_selection.cross_val_predict(\n",
    "    estimator,\n",
    "    X=train_features,\n",
    "    y=train_target,\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# generate a classification report\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e651ae-38cb-49ea-b6b7-6368a959d2aa",
   "metadata": {
    "id": "49e651ae-38cb-49ea-b6b7-6368a959d2aa"
   },
   "source": [
    "Alternatively, with random forests, you can avoid CV altogether by only using a random subset of training samples to fit each tree. The unused training samples can then be used to estimate the validation error for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101ea46-ab9b-4126-96a1-9ddc8b7ed3cc",
   "metadata": {
    "id": "4101ea46-ab9b-4126-96a1-9ddc8b7ed3cc"
   },
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "_classifier_hyperparameters = {\n",
    "    \"bootstrap\": True,\n",
    "    \"max_samples\": 0.9,\n",
    "    \"n_jobs\": -1,\n",
    "    \"oob_score\": True,\n",
    "    \"random_state\": np.random.RandomState(_seed)\n",
    "}\n",
    "\n",
    "estimator = ensemble.RandomForestClassifier(**_classifier_hyperparameters)\n",
    "_ = estimator.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2a19b-5eff-4df7-94a5-efcaed4324af",
   "metadata": {
    "id": "44b2a19b-5eff-4df7-94a5-efcaed4324af"
   },
   "outputs": [],
   "source": [
    "estimator.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e02dfd-16b7-4ad2-ae94-f5192b508335",
   "metadata": {
    "id": "04e02dfd-16b7-4ad2-ae94-f5192b508335"
   },
   "source": [
    "### Exercise: Regularizing Random Forests\n",
    "\n",
    "Our random forest classifier is pretty good. Can tune the behavior of each tree using the same tuning parameters as above. Also can control the number of estimators used in constructing the ensemble: more estimators means a more flexible model. The Scikit Learn documentation has a good discussion on [parameter tunning strategies](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters) for random forest classifiers and regressors. Manually tune the hyperparameters of the random forest classifier to train and find a good set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98600105-39e4-4386-97b7-35ab98d7e127",
   "metadata": {
    "id": "98600105-39e4-4386-97b7-35ab98d7e127"
   },
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "\n",
    "_classifier_hyperparameters = {\n",
    "    \"criterion\": \"entropy\",\n",
    "    \"max_depth\": 16,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"max_samples\": 0.9,\n",
    "    \"min_samples_leaf\": 1e-3,\n",
    "    \"min_samples_split\": 2e-3,\n",
    "    \"n_estimators\": 250,\n",
    "    \"bootstrap\": True,\n",
    "    \"max_samples\": 0.9,\n",
    "    \"n_jobs\": -1,\n",
    "    \"oob_score\": True,\n",
    "    \"random_state\": np.random.RandomState(_seed)\n",
    "}\n",
    "\n",
    "estimator = ensemble.RandomForestClassifier(**_classifier_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10a21d-3a58-48b6-a9a0-8966788b50b6",
   "metadata": {
    "id": "ff10a21d-3a58-48b6-a9a0-8966788b50b6"
   },
   "outputs": [],
   "source": [
    "# make predictions using cv\n",
    "_train_predictions = model_selection.cross_val_predict(\n",
    "    estimator,\n",
    "    X=train_features,\n",
    "    y=train_target,\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# generate a classification report\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f616a7a-5c07-405e-ad8b-32dba5be8bc3",
   "metadata": {
    "id": "2f616a7a-5c07-405e-ad8b-32dba5be8bc3"
   },
   "source": [
    "### Exercise: Exploring Gradient Boosted Trees\n",
    "\n",
    "Read the docs for to understand default behavior of the [`ensemble.HistGradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). Fit a gradient boosted classifier and manually tune the hyperparameters and see if you can outperform your random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d1513c-5972-4f1d-909e-a58bd9be3ff2",
   "metadata": {
    "id": "e0d1513c-5972-4f1d-909e-a58bd9be3ff2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e9aac-2844-4bbb-8a1c-83c373b79413",
   "metadata": {},
   "source": [
    "### Exercise: Voting Classifiers (Challenge!)\n",
    "\n",
    "Read the docs to understand how to create a [`ensemble.VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) using a `svm.LinearSVC`, a `ensemble.RandomForest`, and an `ensemble.HistGradientBoostingClassifier`. Does this classifier out perform the best individual classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ead1ee-dafa-4ddf-85b7-161747adcec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert you code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7255e-4f60-469c-b541-f6935d9cb6e1",
   "metadata": {
    "id": "efe7255e-4f60-469c-b541-f6935d9cb6e1"
   },
   "source": [
    "# Evaluate your models on the test dataset\n",
    "\n",
    "After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. Remember to re-train your model on the full training data prior to evaluating on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809d5ef-16d0-47b5-8d44-87222874ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "]\n",
    "\n",
    "for estimator in estimators:\n",
    "    _ = estimator.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73ca70-1d2b-4053-b500-5d0be0c19f62",
   "metadata": {
    "id": "8a73ca70-1d2b-4053-b500-5d0be0c19f62"
   },
   "outputs": [],
   "source": [
    "for estimator in estimators:\n",
    "\n",
    "    # make predictions\n",
    "    _test_predictions = estimator.predict(test_features)\n",
    "\n",
    "    # generate a classification report\n",
    "    _report = metrics.classification_report(\n",
    "        test_target,\n",
    "        _test_predictions,\n",
    "    )\n",
    "    print(_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55224491-47cc-443d-8592-65fd362e4128",
   "metadata": {
    "id": "55224491-47cc-443d-8592-65fd362e4128"
   },
   "source": [
    "If you did a lot of tuning, the performance will usually be slightly worse than what you measured using cross-validation (because your system ends up fine-tuned to perform well on the validation data and will likely not perform as well on new, unknown datasets). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cedef-b497-4eda-9bbd-ab04263e2087",
   "metadata": {
    "id": "ed2cedef-b497-4eda-9bbd-ab04263e2087"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
