{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea2c52-3ead-4484-8a13-59ff459b0f78",
   "metadata": {
    "id": "e3ea2c52-3ead-4484-8a13-59ff459b0f78"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import requests\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cluster, decomposition, ensemble, linear_model,\n",
    "from sklearn import metrics, model_selection, pipeline, preprocessing, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b241c6-de2e-4c9a-ac4f-6c5a010e2929",
   "metadata": {
    "id": "a9b241c6-de2e-4c9a-ac4f-6c5a010e2929",
    "tags": []
   },
   "source": [
    "# MNIST Dataset\n",
    "\n",
    "The original [MNIST](http://yann.lecun.com/exdb/mnist/) dataset consists of 70000 28x28 black and white images in 10 classes. There are 60000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a812f-ac93-4d77-871d-0d282f51423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might be different if using Colab or Kaggle\n",
    "PROJECT_ROOT_DIR = pathlib.Path(\"..\")\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT_DIR / \"data\" / \"mnist\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT_DIR / \"results\" / \"mnist\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CV_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d2238-7283-441a-b6d8-4ba3c4c682f0",
   "metadata": {
    "id": "e68d2238-7283-441a-b6d8-4ba3c4c682f0"
   },
   "source": [
    "### Download and extract the data (if using Colab or Kaggle!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09916110-f2d8-448c-86ee-4be9ae5be169",
   "metadata": {
    "id": "09916110-f2d8-448c-86ee-4be9ae5be169",
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"https://github.com/KAUST-Academy/practical-tools-for-machine-learning/blob/october-2022/data/mnist/mnist.parquet?raw=true\"\n",
    "\n",
    "with open(DATA_DIR / \"mnist.parquet\", 'wb') as f:\n",
    "    response = requests.get(URL)\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc24c9-260e-4840-8872-723f0adc6f41",
   "metadata": {
    "id": "02cc24c9-260e-4840-8872-723f0adc6f41"
   },
   "source": [
    "### Load the data\n",
    "\n",
    "We will load the data using the [Pandas](https://pandas.pydata.org/) library. Highly recommend the most recent edition of [*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781491957653/) by Pandas creator Wes Mckinney for anyone interested in learning how to use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fded7d-02a7-48c3-99e4-d19d001c8ccf",
   "metadata": {
    "id": "c1fded7d-02a7-48c3-99e4-d19d001c8ccf"
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(DATA_DIR / \"mnist.parquet\")\n",
    "features = data.drop(\"label\", axis=1)\n",
    "target = data.loc[:, \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53a9c4-550d-4577-bf0f-bace74843784",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b53a9c4-550d-4577-bf0f-bace74843784",
    "outputId": "f8441f2a-1945-40fb-9409-bf290c79afd2"
   },
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e51a47-4730-410d-b5d6-4278a48ff5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2212c9-f227-4b8f-ac41-6deb6f714976",
   "metadata": {
    "id": "7a2212c9-f227-4b8f-ac41-6deb6f714976"
   },
   "source": [
    "# Creating a Test Dataset\n",
    "\n",
    "Before we look at the data any further, we need to create a test set, put it aside, and never look at it (until we are ready to test our trainined machine learning model!). Why? We don't want our machine learning model to memorize our dataset (this is called overfitting). Instead we want a model that will generalize well (i.e., make good predictions) for inputs that it didn't see during training. To do this we hold split our dataset into training and testing datasets. The training dataset will be used to train our machine learning model(s) and the testing dataset will be used to make a final evaluation of our machine learning model(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791e74d-cddd-4383-8a6c-5b80df4a5daf",
   "metadata": {
    "id": "f791e74d-cddd-4383-8a6c-5b80df4a5daf"
   },
   "outputs": [],
   "source": [
    "model_selection.train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35bf11-0fa5-4879-bbda-e976244ec929",
   "metadata": {
    "id": "6b35bf11-0fa5-4879-bbda-e976244ec929"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SEED_GENERATOR = np.random.RandomState(SEED)\n",
    "\n",
    "\n",
    "def generate_seed():\n",
    "    return SEED_GENERATOR.randint(np.iinfo(\"uint16\").max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff5a36-07c7-404f-916b-260c789aff91",
   "metadata": {
    "id": "beff5a36-07c7-404f-916b-260c789aff91"
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and testing data\n",
    "_seed = generate_seed()\n",
    "_random_state = np.random.RandomState(_seed)\n",
    "\n",
    "train_features, test_features, train_target, test_target = model_selection.train_test_split(\n",
    "    features,\n",
    "    target,\n",
    "    test_size=1e-1,\n",
    "    random_state=_random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed741c-996c-4df3-8680-49f14cc0dcf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9ed741c-996c-4df3-8680-49f14cc0dcf0",
    "outputId": "9d66db77-3063-4cfb-8b92-05118d8ddcf8"
   },
   "outputs": [],
   "source": [
    "train_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f35a7-604c-486b-8f36-284b909c64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090af9ae-e36c-4c5d-ac2d-25e1ea1c34df",
   "metadata": {
    "id": "090af9ae-e36c-4c5d-ac2d-25e1ea1c34df"
   },
   "source": [
    "Again, if you want to you can write out the train and test sets to disk to avoid having to recreate them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede1948-a85a-450f-8c0b-8c18b56ed4d3",
   "metadata": {
    "id": "6ede1948-a85a-450f-8c0b-8c18b56ed4d3"
   },
   "outputs": [],
   "source": [
    "_ = (train_features.join(train_target)\n",
    "                   .to_parquet(DATA_DIR / \"train.parquet\", index=False))\n",
    "\n",
    "_ = (test_features.join(test_target)\n",
    "                   .to_parquet(DATA_DIR / \"test.parquet\", index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc80e6-5e4b-42dc-8584-7dfdb397cb27",
   "metadata": {
    "id": "54c62f5c-21cd-4138-8776-baca812ba3d9",
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is one of the most important parts of any machine learning project. There are two main tasks in feature engineering.\n",
    "\n",
    "* Feature selection: selecting the best subset of features for training. \n",
    "* Feature extraction: combining existing features to produce new features for training.\n",
    "* Feature creation: finding additional data sources to use as features.\n",
    "\n",
    "Feature engineering is often the most labor intensive part of building a machine learning pipeline and often requires extensive expertise/domain knowledge relevant to the problem at hand. Recently packages such as [featuretools](https://www.featuretools.com/) have been developed to (partially) automate the process of feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5391747-67b0-4975-bb9f-9f80e6d2682b",
   "metadata": {
    "id": "IydYfnmRtEDY",
    "tags": []
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277c037-3bee-491e-9ccd-d34ebcd41ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features.std(axis=0)\n",
    "               .describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dd78d-6be0-47d0-8f87-a951f68504fd",
   "metadata": {
    "id": "apDDcKsbq4Z6"
   },
   "outputs": [],
   "source": [
    "minimum_threshold = 0 # maybe something greater than 0?\n",
    "useful_train_pixels = train_features.std(axis=0) > minimum_threshold\n",
    "useful_train_features = train_features.loc[:, useful_train_pixels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbfda8a-2f9f-4317-b731-a6910b633e65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFesWIA-ryY7",
    "outputId": "f382fda5-8d1d-4b09-ab7e-dae5463d6868"
   },
   "outputs": [],
   "source": [
    "useful_train_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a08ef-541a-4775-a414-9dd8c9d81806",
   "metadata": {
    "id": "87310b3d-bd8c-47e0-be44-1a7206bcc845"
   },
   "source": [
    "### Feature extraction using Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd8e19-05df-4ff8-9796-cb95697bd099",
   "metadata": {
    "id": "c47345e5-5f5e-4b81-aa4c-c74757d98bb2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "decomposition.PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ce9d7-edec-41a6-82a5-effec0b6a288",
   "metadata": {
    "id": "cffe9c01-03e6-44f4-a803-faf3f939fe9e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "\n",
    "# hyper-parameters\n",
    "_pca_hyperparameters = {\n",
    "    \"n_components\": 0.95,\n",
    "    \"whiten\": False,\n",
    "}\n",
    "\n",
    "feature_extractor = decomposition.PCA(**_pca_hyperparameters)\n",
    "extracted_train_features = feature_extractor.fit_transform(useful_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544fd74-c980-4a44-bb3c-73ac31f44bd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afff6d94-d47a-4e8f-b958-490924398545",
    "outputId": "aeb9373a-5e75-4985-d509-a2f97eacf795"
   },
   "outputs": [],
   "source": [
    "extracted_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09733cb7-d31a-4211-ac4e-cbc16ce2a3e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71b1db02-41f7-4fc3-80c2-4e8e5226c1bf",
    "outputId": "c96fc67a-69fb-4de4-9e09-696facee0254"
   },
   "outputs": [],
   "source": [
    "extracted_train_features[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085ca29-c57a-4f14-9db7-50682791d1da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f166dc3-3c42-470c-aa87-b6cece88fc21",
    "outputId": "af4780c6-b686-4828-884f-3483115f2626"
   },
   "outputs": [],
   "source": [
    "extracted_train_features.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880fbb7-46b8-4b8e-be84-70acbf3276bd",
   "metadata": {
    "id": "40548043-1900-4790-b1f6-06a156383260"
   },
   "outputs": [],
   "source": [
    "extracted_train_features.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17e2a7-a20d-4f2a-b623-40d16589cde2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "id": "91e6542c-bcdd-425f-8ce6-eea4b3f7bd45",
    "outputId": "2d9b8e88-9a07-4877-902f-f09f0979879f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "_ = ax.scatter(\n",
    "    extracted_train_features[:, 0],\n",
    "    extracted_train_features[:, 1],\n",
    "    c=train_target,\n",
    "    alpha=0.05\n",
    ")\n",
    "_ = ax.set_xlabel(\"Component 1\", fontsize=15)\n",
    "_ = ax.set_ylabel(\"Component 2\", fontsize=15)\n",
    "_ = ax.set_title(type(feature_extractor))\n",
    "_ = ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261acdf-b457-4478-9b0a-6b25d69d5638",
   "metadata": {
    "id": "54c7ec60-138c-4be1-9934-8eae3089b1f5"
   },
   "source": [
    "### Exercise: To whiten, or not to whiten?\n",
    "\n",
    "Take a close look at the doc string for the `decomposition.PCA` algorithm. What happens if you set `n_components` to a number between 0 and 1 (i.e., `n_components=0.95`)? Why might you want to do this? What does setting `whiten=True` do to the output of the algorithm? Re-run the PCA algorithm above setting `whiten=True` to confirm your answer. Why might you want to set `whiten=True`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a2bf5-9f1a-45e5-9ace-c9725e71e161",
   "metadata": {
    "id": "39a98ccd-0eb1-433f-af72-49d43eb79925"
   },
   "outputs": [],
   "source": [
    "# insert code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c890cd-ffb5-4f15-9498-20858c827b83",
   "metadata": {},
   "source": [
    "### Feature extraction using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746cf91-6acf-48fc-a874-d4aa402c7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "\n",
    "_extractor_hyperparameters = {\n",
    "    \"n_clusters\": 10,\n",
    "    \"random_state\": np.random.RandomState(_seed),\n",
    "}\n",
    "\n",
    "feature_extractor = cluster.KMeans(**_extractor_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b41e8e-2bcf-46e8-ba2b-c7ac9468a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_train_features = feature_extractor.fit_transform(useful_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9fb6be-1625-489c-b479-2e41e7263aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b292be-f7b9-4958-a6d0-c957ebcc50a5",
   "metadata": {
    "id": "01b292be-f7b9-4958-a6d0-c957ebcc50a5"
   },
   "source": [
    "# Select, train, and validate some pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab8454-92c2-46f9-9c60-ac0f5855bedd",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335899f-8bbd-4bb8-a411-d68da1ad8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.Perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792c574-ebd3-4d5c-9ed3-5b8db288b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "\n",
    "_classifier_hyperparameters = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": np.random.RandomState(_seed),\n",
    "}\n",
    "\n",
    "ml_pipeline = pipeline.make_pipeline(\n",
    "    # insert code here!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc1e21-8f3f-410c-a0cf-4aa981e59891",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ml_pipeline.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4d7c8-cac3-4d72-ac9c-41a0df173775",
   "metadata": {},
   "source": [
    "After training the pipeline we can assess its performance on the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7c6d8-a208-4e04-bda7-62c1f35bdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "_train_predictions = ml_pipeline.predict(train_features)\n",
    "\n",
    "# report the accuracy on the training data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1b1b0-dfb2-451b-99dd-c993d22c5e6e",
   "metadata": {},
   "source": [
    "...and then assess its performance on new data using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c53d46-5648-4e24-be70-356971df9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_FOLDS = 5\n",
    "\n",
    "_train_predictions = model_selection.cross_val_predict(\n",
    "    ml_pipeline,\n",
    "    X=train_features,\n",
    "    y=train_target,\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# report the accuracy on the cv data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8999df08-6848-44a8-9699-47c21ca0b5c8",
   "metadata": {},
   "source": [
    "You can also use SGD to fit perceptrons as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60b224-6d20-4a71-92fe-f5cdc7be9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.SGDClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f86923-ce8c-4f57-b8d7-7e7d1d710a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "\n",
    "_classifier_hyperparameters = {\n",
    "    \"loss\": \"perceptron\",\n",
    "    \"fit_intercept\": True,\n",
    "    \"verbose\": 0,\n",
    "    \"random_state\": np.random.RandomState(_seed),\n",
    "}\n",
    "\n",
    "ml_pipeline = pipeline.make_pipeline(\n",
    "    # insert code here!\n",
    "    linear_model.SGDClassifier(**_classifier_hyperparameters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2f25a-e712-466e-a58d-3ac7204f5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ml_pipeline.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a794672-7edc-4d89-9974-7ddabe744cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "_train_predictions = ml_pipeline.predict(train_features)\n",
    "\n",
    "# report the accuracy on the training data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726094fa-65cf-4188-8283-ce62d4da5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_predictions = model_selection.cross_val_predict(\n",
    "    ml_pipeline,\n",
    "    X=train_features,\n",
    "    y=train_target,\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# report the accuracy on the cv data\n",
    "_report = metrics.classification_report(\n",
    "    train_target,\n",
    "    _train_predictions,\n",
    ")\n",
    "print(_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b32ff9-ae55-46c4-ad21-528413893ac0",
   "metadata": {},
   "source": [
    "### Exercise: Early Stopping\n",
    "\n",
    "Read through the documentation for the `linear_model.SGDClassifier` and implement a Perceptron and train the model using early stopping to control overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c8b6b-fac7-4a38-b7fe-5e2a8c99cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.SGDClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5815d4-941e-4790-8426-ec9e56b531aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8456574-cca4-43b1-8a49-87585bfe73e7",
   "metadata": {},
   "source": [
    "### Exercise: Feature engineering and Gradient Boosted Trees\n",
    "\n",
    "Combine feature selection, feature engineering usin PCA with Gradient Boosted Trees. How does this compare in terms of computation speed, and accuracy to other methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1b2f1-bb5c-40ed-912b-513d06e2c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7255e-4f60-469c-b541-f6935d9cb6e1",
   "metadata": {
    "id": "efe7255e-4f60-469c-b541-f6935d9cb6e1"
   },
   "source": [
    "# Evaluate your models on the test dataset\n",
    "\n",
    "After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. Remember to re-train your model on the full training data prior to evaluating on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809d5ef-16d0-47b5-8d44-87222874ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ???\n",
    "]\n",
    "\n",
    "for estimator in estimators:\n",
    "    _ = estimator.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73ca70-1d2b-4053-b500-5d0be0c19f62",
   "metadata": {
    "id": "8a73ca70-1d2b-4053-b500-5d0be0c19f62"
   },
   "outputs": [],
   "source": [
    "for estimator in estimators:\n",
    "\n",
    "    # make predictions\n",
    "    _test_predictions = estimator.predict(test_features)\n",
    "\n",
    "    # generate a classification report\n",
    "    _report = metrics.classification_report(\n",
    "        test_target,\n",
    "        _test_predictions,\n",
    "    )\n",
    "    print(estimator)\n",
    "    print(_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55224491-47cc-443d-8592-65fd362e4128",
   "metadata": {
    "id": "55224491-47cc-443d-8592-65fd362e4128"
   },
   "source": [
    "If you did a lot of tuning, the performance will usually be slightly worse than what you measured using cross-validation (because your system ends up fine-tuned to perform well on the validation data and will likely not perform as well on new, unknown datasets). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cedef-b497-4eda-9bbd-ab04263e2087",
   "metadata": {
    "id": "ed2cedef-b497-4eda-9bbd-ab04263e2087"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
